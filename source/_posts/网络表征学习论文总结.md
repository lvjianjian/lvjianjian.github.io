---
title: 网络表征学习论文总结
date: 2018-08-14 23:36:59
tags: [NRL, NE, 网络表征]
categories: [机器学习]
---


Embedding真的是一个很神奇的东西，自动word2vec出世之后，感觉一切东西都在想办法做embedding。其实我觉得就是应该这样，在计算机的世界只有数字，因此如何表示一个东西也只能靠数字，而embedding正是让我们往这个方向靠近，合理的embedding显得尤为重要。

这里简单总结下看到的NE/NRL的embedding方法，会持续更新。

| Title | Author | Venue |Contribution| 
|:-----|-|-|:------------------|
|DeepWalk: Online Learning of Social Representations|Bryan Perozzi, Rami Al-Rfou, Steven Skiena| KDD 2014| 1. 使用random walk来对graph进行序列采样。<br> 2. 然后用skip-gram算法来对采样出的序列进行embedding。思想很简单，但是确实很有效。|
|node2vec: Scalable Feature Learning for Networks|Aditya Grover, Jure Leskovec| KDD 2016|1. 相比较于DeepWalk, 定义了一个bias随机游走。DFS和BFS对于学到的结构信息是不一样的，BFS更注重局部信息的挖掘，DFS会在图中走得更深，更注重全局结构信息。<br>2. 具体定义上，从t到v，返回t的未归一概率为1/p，到与t邻接的节点未归一概率为1，到与t不邻接的节点未归一概率为1/q, q控制着DFS/BFS的程度，p控制着返回原节点的程度。<br>3. 然后在采样完之后仍然使用skip-gram。|
|Network Representation Learning with Rich Text Information|Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, Edward Y. Chang|IJCAI 2015|1. 论文说明了DeepWalk等同于M的矩阵分解，M<sub>i,j</sub> 表示从i节点不超过k步到大j节点的概率。<br>2. 因此该论文直接是用矩阵分解求向量，并且该论文还引入了节点信息，也是就再用一个矩阵  T  表示节点的额外信息, 最后优化 (M - W<sup>T</sup>HT) 的 F范数 加上 W 和 H 的L2 正则之和|
|Max-Margin DeepWalk Discriminative Learning of Network Representation|Cunchao Tu, Weicheng Zhang, Zhiyuan Liu, Maosong Sun|IJCAI 2016|1. DeepWalk 结合 SVM，损失函数正是两者的和 <br> 2. 先固定分解矩阵优化svm部分，再固定svm部分优化分解的矩阵，这里优化时候加入了一个偏置梯度。（其实不是很明白，以后再过这一篇）|
|GraRep: Learning Graph Representations with Global Structural Information|	Shaosheng Cao, Wei Lu, Qiongkai Xu|CIKM 2015|分析不同k步到达所刻画的图信息不同，并对k步到达转移概率矩阵做矩阵分解来学习k步到达的图的节点向量。最后将1 到 K 的向量连接成最后的向量表示。|
|LINE: Large-scale Information Network Embedding|Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, Qiaozhu Mei|WWW 2015|1. 1st order proximity 直接优化i和j的相似度，以归一化后的权重为目标，使用KL散度作为loss函数。 <br> 2. 2st order proximity 是优化i节点确定下j节点的概率，以i和j的权重除以i节点出度之和为目标，仍然使用KL散度。 <br> 3. 采用边的负采样方法，本质就是skip-gram的负采样。|
|PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks|Jian Tang, Meng Qu, Qiaozhu Mei|KDD 2015|1. 类似LINE的2st order proximity应用在二分图网络节点。 <br> 2. 构建word-word，word-document，word-label 3种二分图网络，最后的损失函数为三者之和。 <br> 3. 提出两种优化方式，一种直接三者一起优化，一种先优化word-word和word-document，最后优化word-label损失 |
|Fast Network Embedding Enhancement via High Order Proximity Approximation|Cheng Yang, Maosong Sun, Zhiyuan Liu, Cunchao Tu|IJCAI 2017|1. 在低阶(low-order)矩阵分解的结果上进行更新以获得高阶(high-order)效果。 <br> 2. 对所有矩阵分解类的算法都适用，如果我们分解出的矩阵为R和C，更新法则为R<sub>new</sub>=kAR,C<sub>new</sub>=kA<sup>T</sup>C,其中A为1步转移概率矩阵，k需要小于0.5|
|TransNet: Translation-Based Network Representation Learning for Social Relation Extraction|Cunchao Tu, Zhengyan Zhang, Zhiyuan Liu, Maosong Sun|IJCAI 2017|1. 这篇文章不但可以对node编码，其实还对边上的label进行了编码，使用一个autoencode对边的labels进行编码，并且限定起始点的vector加上该边的vector等于终点的vector。<br> 2. 对节点进行了编码。并且可以通过终点向量减去起点向量并使用AE的解码部分来预测边上的labels。 |
|A General Framework for Content-enhanced Network Representation Learning|Xiaofei Sun, Jiang Guo, Xiao Ding, Ting Liu|CoRR 2016|1. 学习节点带有文本信息的向量表示化，在paper中文本指句子。<br> 2. 损失函数由2部分组成，第一部分是正常的节点和节点的损失函数，第二部分是节点和内容的损失函数。<br> 3. 内容的表征用RNN/BiRNN编码|
|Context-Aware Network Embedding for Relation Modeling|Cunchao Tu, Han Liu, Zhiyuan Liu, Maosong Sun|ACL 2017|1. 学习节点的结构向量和文本向量 <br> 2.  文本向量的学习通过节点文本的cnn操作出的2个矩阵，在中间乘上一个attentive矩阵(可能只是目标的空间变换)，再做行、列的池化加归一化得到两个节点的attention向量，最后乘以cnn出的矩阵得到文本向量。<br> 3. 损失函数由u，v的结构、文本的4种条件概率组成（Line中的二阶相似）,使用负采样算法 