---
title: 网络表征学习论文总结
date: 2018-08-14 23:36:59
tags: [NRL, NE, 网络表征]
categories: [机器学习]
---


Embedding真的是一个很神奇的东西，自动word2vec出世之后，感觉一切东西都在想办法做embedding。其实我觉得就是应该这样，在计算机的世界只有数字，因此如何表示一个东西也只能靠数字，而embedding正是让我们往这个方向靠近，合理的embedding显得尤为重要。

这里简单总结下看到的NE/NRL的embedding方法，会持续更新。

| Title | Author | Venue |Contribution| 
|:-----|-|-|:------------------|
|DeepWalk: Online Learning of Social Representations|Bryan Perozzi, Rami Al-Rfou, Steven Skiena| KDD 2014| 1. 使用random walk来对graph进行序列采样。<br> 2. 然后用skip-gram算法来对采样出的序列进行embedding。思想很简单，但是确实很有效。|
|node2vec: Scalable Feature Learning for Networks|Aditya Grover, Jure Leskovec| KDD 2016|1. 相比较于DeepWalk, 定义了一个bias随机游走。DFS和BFS对于学到的结构信息是不一样的，BFS更注重局部信息的挖掘，DFS会在图中走得更深，更注重全局结构信息。<br>2. 具体定义上，从t到v，返回t的未归一概率为1/p，到与t邻接的节点未归一概率为1，到与t不邻接的节点未归一概率为1/q, q控制着DFS/BFS的程度，p控制着返回原节点的程度。<br>3. 然后在采样完之后仍然使用skip-gram。|
|Network Representation Learning with Rich Text Information|Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, Edward Y. Chang|IJCAI 2015|1. 论文说明了DeepWalk等同于M的矩阵分解，M<sub>i,j</sub> 表示从i节点不超过k步到大j节点的概率。<br>2. 因此该论文直接是用矩阵分解求向量，并且该论文还引入了节点信息，也是就再用一个矩阵  T  表示节点的额外信息, 最后优化 (M - W<sup>T</sup>HT) 的 F范数 加上 W 和 H 的L2 正则之和|
|Max-Margin DeepWalk Discriminative Learning of Network Representation|Cunchao Tu, Weicheng Zhang, Zhiyuan Liu, Maosong Sun|IJCAI 2016||
|Learning Graph Representations with Global Structural Information|	Shaosheng Cao, Wei Lu, Qiongkai Xu|CIKM 2015||
